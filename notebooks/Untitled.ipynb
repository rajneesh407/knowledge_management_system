{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7d4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from typing import Tuple, Iterator\n",
    "from typing import Sequence, List, Optional\n",
    "from langchain.schema import BaseStore\n",
    "from pypika import Query, Table, Field, Column\n",
    "\n",
    "\n",
    "class ChromaStore(BaseStore[str, bytes]):\n",
    "\n",
    "    def __init__(self, path, user_id):\n",
    "        self.path = path\n",
    "        self.table_name = \"docstore_{}\".format(user_id)\n",
    "        self.table = Table(self.table_name)\n",
    "        self.id_column = Field('id')\n",
    "        self.data_column = Field('data')\n",
    "        self._create_table()\n",
    "\n",
    "    def get_connection(self):\n",
    "        return sqlite3.connect('{path}/chroma.sqlite3'.format(path=self.path))\n",
    "\n",
    "    def _create_table(self):\n",
    "        id_column = Column('id', 'VARCHAR(50)', nullable=False)\n",
    "        data_column = Column('data', 'VARCHAR(2500)', nullable=False)\n",
    "        create_table_query = Query.create_table(self.table).columns(id_column, data_column).if_not_exists()\n",
    "        with self.get_connection() as connection:\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(create_table_query.get_sql())\n",
    "            cursor.close()\n",
    "\n",
    "    def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:\n",
    "        select_query = Query.from_(self.table).select(self.data_column).where(self.id_column.isin(keys))\n",
    "        with self.get_connection() as connection:\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(select_query.get_sql())\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            cursor.close()\n",
    "\n",
    "            data_list = []\n",
    "            for result in results:\n",
    "                if result[0] is not None:\n",
    "                    data_list.append(json.loads(result[0]).encode(\"utf-8\"))\n",
    "                else:\n",
    "                    data_list.append(None)\n",
    "\n",
    "            return data_list\n",
    "\n",
    "    def mset(self, key_value_pairs: Sequence[Tuple[int, bytes]]) -> None:\n",
    "        insert_queries = []\n",
    "        for key, value in key_value_pairs:\n",
    "            insert_query = Query.into(self.table).columns(self.id_column, self.data_column).insert(key, json.dumps(\n",
    "                value.decode('utf-8')))\n",
    "            insert_queries.append(insert_query)\n",
    "        with self.get_connection() as connection:\n",
    "            cursor = connection.cursor()\n",
    "            for query in insert_queries:\n",
    "                cursor.execute(query.get_sql())\n",
    "            connection.commit()\n",
    "            cursor.close()\n",
    "\n",
    "    def mdelete(self, keys: Sequence[int]) -> None:\n",
    "        delete_query = Query.from_(self.table).delete().where(self.id_column.isin(keys))\n",
    "        with self.get_connection() as connection:\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(delete_query.get_sql())\n",
    "            connection.commit()\n",
    "            cursor.close()\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterator[str]:\n",
    "        select_query = Query.from_(self.table).select(self.id_column)\n",
    "        if prefix:\n",
    "            select_query = select_query.where(self.id_column.like(f'{prefix}%'))\n",
    "        with self.get_connection() as connection:\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(select_query.get_sql())\n",
    "\n",
    "            for row in cursor.fetchall():\n",
    "                yield row[0]\n",
    "\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e4961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajneesh.jha\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=embedding_function, persist_directory=\"../data/\")\n",
    "cs = ChromaStore(\"../data/\", \"summaries\")\n",
    "\n",
    "store = create_kv_docstore(cs)\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aa2f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heelo\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "client = chromadb.Client(Settings(is_persistent=True,\n",
    "                                    persist_directory=\"../data/\",\n",
    "                                ))\n",
    "# coll = client.list(\"sd\")\n",
    "\n",
    "if \"summaries\" in [c.name for c in client.list_collections()]:\n",
    "    print(\"Heelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919145f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=summaries)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccadde6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\n",
    "    \"Please Explain Attention Mechanism\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515b20f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'doc_id': '6a9d4db7-a0a2-462c-9127-7dc1d8b7b355'}, page_content='The image presents a flowchart illustrating the process of using Scaled Dot-Product Attention (SDPA) to combine the outputs of multiple attention heads in a neural network.\\n\\n** Overview **\\nThe flowchart is divided into several sections, each representing a different stage in the process.\\n\\n** Main Points **\\n\\n* **Input**\\n\\t+ Three input vectors: V, K, and Q\\n\\t+ Each vector has an arrow pointing to a box labeled \"Linear\"\\n* **Linear Transformation**\\n\\t+ Each'),\n",
       "  0.9818795248202128),\n",
       " (Document(metadata={'doc_id': 'bf818020-71c4-4575-a026-7d40aaafe723'}, page_content='The image presents a diagram illustrating the architecture of two neural network models: Scaled Dot-Product Attention and Multi-Head Attention. The diagram is divided into two sections, with the left side representing Scaled Dot-Product Attention and the right side representing Multi-Head Attention.\\n\\n**Scaled Dot-Product Attention**\\n\\n*   This section is titled \"Scaled Dot-Product Attention\" and features a series of boxes connected by arrows, indicating the flow of data between components.\\n*   The boxes are labeled with'),\n",
       "  0.9875780277064253),\n",
       " (Document(metadata={'doc_id': '96752674-0273-405c-8361-7233cc407358'}, page_content='**Transformer Attention Applications**\\n\\n**Encoder-Decoder Attention**\\n\\n* Queries from decoder previous layer\\n* Memory keys and values from encoder output\\n* Each position in decoder attends over all positions in input sequence\\n* Mimics typical encoder-decoder attention mechanisms in sequence-to-sequence models\\n\\n**Self-Attention Layers in Encoder**\\n\\n* All keys, values, and queries from same place: encoder output\\n* Each position attends to all positions in previous encoder layer\\n\\n**Self-Attention Layers in Decoder**\\n\\n* Each position attends to all positions in decoder up to and including that position\\n* Prevent leftward information flow in decoder to preserve auto-regressive property\\n* Implemented through scaled dot-product attention with masking of illegal connections'),\n",
       "  1.0258474502379682),\n",
       " (Document(metadata={'doc_id': 'b06b77aa-8c9a-4832-ac04-3e338f5d7e31'}, page_content='Here is the summary:\\n\\n**Attention**\\n\\n* Maps a query and a set of key-value pairs to an output\\n* Query, keys, values, and output are all vectors\\n* Output is a weighted sum of the values, where weights are computed by a compatibility function of the query with the corresponding key\\n\\n**Scaled Dot-Product Attention**\\n\\n* Input consists of queries and keys of dimension dk and values of dimension dv\\n* Compute dot products of the query vectors with the key vectors\\n* Apply softmax function to obtain weights on the query with all keys\\n* Divide each weighted value by the value vector\\n\\n**Mathematical Representation**\\n\\n* Compute matrix of outputs as: Attention(Q, K, V) = softmax(QK^T √dk)V\\n\\n**Comparison with Additive Attention**\\n\\n* Additive attention computes compatibility function using a feed-forward network with a single hidden layer\\n* Dot-product attention is faster and more space-efficient in practice\\n* For small values of dk, the two mechanisms perform similarly, but additive attention outperforms dot product attention without scaling for larger values of dk\\n* Scaling dot products by 1/√dk helps to counteract growing magnitude of dot products for large values of dk'),\n",
       "  1.0328692311923373)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search_with_score(\"Please Explain Attention Mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29f23e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StrOutputParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 72\u001b[0m\n\u001b[0;32m     57\u001b[0m     output \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     58\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     59\u001b[0m         messages\u001b[38;5;241m=\u001b[39mprompt_list,\n\u001b[0;32m     60\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     65\u001b[0m chain_with_sources \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retriever \u001b[38;5;241m|\u001b[39m RunnableLambda(parse_docs),\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough(),\n\u001b[0;32m     68\u001b[0m } \u001b[38;5;241m|\u001b[39m RunnablePassthrough()\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m     69\u001b[0m     response\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     70\u001b[0m         RunnableLambda(build_prompt)\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;241m|\u001b[39m RunnableLambda(genai_model)\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     73\u001b[0m     )\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     76\u001b[0m response \u001b[38;5;241m=\u001b[39m chain_with_sources\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain Attention Mechanism\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StrOutputParser' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from base64 import b64decode\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc.page_content)\n",
    "            b64.append(doc.page_content)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "    \n",
    "    \n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.page_content\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the images. \n",
    "    \n",
    "    Context: {context_text}\n",
    "    \n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "    return [{\"role\":\"user\",\"content\":prompt_content}]\n",
    "def genai_model(prompt_list):\n",
    "    output = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "        messages=prompt_list,\n",
    "        max_tokens=1200\n",
    "    )\n",
    "    \n",
    "    return output.choices[0].message.content\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | RunnableLambda(genai_model)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")\n",
    "\n",
    "response = chain_with_sources.invoke(\n",
    "    \"Explain Attention Mechanism\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "#     print(text.text)\n",
    "    print(\"Page number: \", text.metadata['page_number'])\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "for image in response['context']['images']:\n",
    "    display_base64_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac28676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.docx import partition_docx\n",
    "\n",
    "class WordDocParser:\n",
    "    def __init__(self, docx_filename):\n",
    "        self.docx_filename = docx_filename\n",
    "\n",
    "    def parse_docx(self):\n",
    "        docx_path = self.docx_filename\n",
    "        chunks = partition_docx(\n",
    "            filename=docx_path,\n",
    "            infer_table_structure=True,\n",
    "            strategy=\"hi_res\",\n",
    "            extract_image_block_types=[\"Image\"],\n",
    "            extract_image_block_to_payload=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=4000,\n",
    "            combine_text_under_n_chars=1000,\n",
    "            new_after_n_chars=3000,\n",
    "        )\n",
    "\n",
    "        tables_list = []\n",
    "        texts_list = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            if \"Table\" in str(type(chunk)):\n",
    "                tables_list.append(chunk)\n",
    "            if \"CompositeElement\" in str(type(chunk)):\n",
    "                texts_list.append(chunk)\n",
    "\n",
    "        images_list = self._get_images_base64(chunks)\n",
    "        return texts_list, tables_list, images_list\n",
    "\n",
    "    def _get_images_base64(self, chunks):\n",
    "        images_b64 = []\n",
    "        for chunk in chunks:\n",
    "            if \"CompositeElement\" in str(type(chunk)):\n",
    "                chunk_els = chunk.metadata.orig_elements\n",
    "                for el in chunk_els:\n",
    "                    if \"Image\" in str(type(el)):\n",
    "                        images_b64.append(el.metadata.image_base64)\n",
    "        return images_b64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb5ee7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "az = WordDocParser(r\"C:\\Users\\rajneesh.jha\\Downloads\\GGX Demo Details V1.docx\").parse_docx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc0718f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<unstructured.documents.elements.CompositeElement at 0x19968823d90>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x199688212d0>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x19968821590>],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64e26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e498884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGX Demo:\\n\\nOverview of GGX:\\n\\nIntroduction to GGX project and platform\\n\\nRisks associated with the GenAi Pipeline\\n\\nRegulatory requirements for GenAI pipelines\\n\\nData Vault:\\n\\nA place to host all the useful datasets and track it within Corridor environment.\\n\\nAbility to connect to the client’s data lake or upload custom datasets.\\n\\nRun pre configured multiple data related checks.\\n\\nGGX curated validation datasets for testing/validation of different GGX components.\\n\\nOpen Source datasets already present in the platform for some of the famous benchmarking tests.\\n\\nIntent Recognition Validation Data: https://genai2.corridorplatforms.com/data-vault/table-registry/35/details\\n\\nResponse Validation Data: https://genai2.corridorplatforms.com/data-vault/table-registry/37/details\\n\\nGGX Prompt Injection Strategies:\\n\\nhttps://genai2.corridorplatforms.com/data-vault/table-registry/38/details\\n\\nGenAi Studio:\\n\\nCentral place to organize all the GenAI related work with proper access controls in place.\\n\\nModular components for registering/validating and monitoring different parts of GenAI pipelines. (Models, Prompts, RAG)\\n\\nFinally create pipelines by combining different components for specific use cases and perform multiple tests on it both automated + human integrated.\\n\\nModel Catalog:\\n\\nA central place to register and test all the Foundational and Fine-Tuned models\\n\\nSimple registration process to put the exact logic and documentation for the model and decide the use-cases it would be permissioned for.\\n\\nAbility to connect to external systems using APIs or upload the model artifact within the Corridor environment.\\n\\nFingerprinting to govern the external APIs.\\n\\nAdditional capability to track changes, approvals, version control, reviews and monitoring, impact assessment, approve it once and use it across multiple projects avoiding cycles of testing+approval.\\n\\nAbility to test the components using various pre-packaged tests on the platform or create custom tests using a nice UI like interface .(At the end show the Reporting section after HIT maybe)\\n\\nShow toxicity evaluation and Show Gender Bias and Gender Bias with Prompt variations.\\n\\nShare the object documentation + all the tests with reviewers and get their feedback to do continuous improvements.\\n\\nGPT4 Model with all the Comparison Dashboard (with GPT3.5 and LLAMA): https://genai2.corridorplatforms.com/genai-studio/model-catalog/2/details\\n\\nPrompts:\\n\\nA store to register, track and monitor all the prompts.\\n\\nGGX curated pre-packaged prompts evaluation metrics to test use-case specific prompts quality.\\n\\nAbility to test the prompt individually or Ability to test it by combining with other components like Model to evaluate prompt in relation to the outcome.\\n\\nShow the charts for prompt quality and prompt classification accuracy.\\n\\nIntent Recognition Prompt (With Dashboards): https://genai2.corridorplatforms.com/genai-studio/prompt-registry/53/details\\n\\nResponse Prompt : https://genai2.corridorplatforms.com/genai-studio/prompt-registry/52/details\\n\\nRetrieval Augmented Generation (RAG):\\n\\nAbility to connect to an external knowledge database to augment LLM’s knowledge with use-case specific information.\\n\\nAbility to upload the Knowledge base and track it within the corridor or connect through external APIs with proper fingerprinting.\\n\\nShow dashboards\\n\\nRAG (With Dashboard) : https://genai2.corridorplatforms.com/genai-studio/rag-registry/14/details\\n\\nPipeline Builder:\\n\\nA place to build and test your use-case specific pipelines\\n\\nCombine one or more components from Models, Prompts and RAG in a use-case specific manner to stitch the end to pipeline together using very minimal code.\\n\\nAbility to quickly swap in and swap out any component and compare multiple pipelines to export the best one for production.\\n\\nView the complete lineage of your pipelines, any permissible purpose violations etc.\\n\\nTest the pipeline using multiple use-case specific reports and take it through the approval process.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "607aea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import pygame\n",
    "from io import BytesIO\n",
    "\n",
    "# The text that you want to convert to audio\n",
    "mytext = \"\"\"Attention in this context refers to an attention function, which is a mechanism that maps a query and a set of key-value pairs to an output. The input consists of queries and keys of dimension dk, and values of dimension dv. The attention function computes the dot products of the queries with all keys, applies a softmax function to obtain the weights on the query with all keys, divides each by √ dk, and then multiplies the values with these weights. This process is also known as Scaled Dot-Product Attention.\n",
    "\n",
    "Attention is a crucial component of various deep learning models, including the Transformer model. It allows the model to focus on specific parts of the input data, weigh them appropriately, and produce an output based on these weighted inputs. In the context of the Transformer model, attention is used in multiple layers, including encoder-decoder attention layers, self-attention layers in the encoder, and self-attention layers in the decoder. It plays a key role in enhancing the model's ability to capture complex relationships between different elements of the input data.\n",
    "\n",
    "In summary, attention refers to the attention function used in deep learning models, which computes the weighted sum of values based on their compatibility with queries, as determined by the dot product of the queries with keys, normalized by a scaling factor.\"\"\"\n",
    "\n",
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "\n",
    "# Passing the text and language to the engine\n",
    "tts = gTTS(text=mytext, lang=language, slow=False)\n",
    "\n",
    "# Create an in-memory bytes buffer to save the audio\n",
    "audio_stream = BytesIO()\n",
    "tts.write_to_fp(audio_stream)\n",
    "\n",
    "# Rewind the buffer to the beginning\n",
    "audio_stream.seek(0)\n",
    "\n",
    "# Initialize the mixer module\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load the mp3 data from the bytes stream\n",
    "pygame.mixer.music.load(audio_stream, \"mp3\")\n",
    "\n",
    "# Play the loaded mp3 data\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "# Keep the program running until the audio finishes playing\n",
    "while pygame.mixer.music.get_busy():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbb485b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise... Please wait.\n",
      "Listening for your speech...\n",
      "Processing audio...\n",
      "You said: what is attention\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Use the microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Adjusting for ambient noise... Please wait.\")\n",
    "    recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "    print(\"Listening for your speech...\")\n",
    "    try:\n",
    "        audio = recognizer.listen(source)\n",
    "        text = recognizer.recognize_google(audio)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, I could not understand the audio.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error with Google Speech Recognition service: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60e455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
